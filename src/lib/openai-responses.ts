import { createOpenAIClient } from './openai-client'
import { createSupabaseAdmin } from '@/lib/supabase'

let cachedSystemInstructions: { text: string | null; fetchedAt: number } | null = null
const INSTRUCTIONS_CACHE_TTL_MS = 60_000

// Expose helpers to allow other modules (e.g., admin settings API) to refresh cache immediately
export function setSystemInstructionsCache(text: string | null) {
  cachedSystemInstructions = { text, fetchedAt: Date.now() }
}

export function invalidateSystemInstructionsCache() {
  cachedSystemInstructions = null
}

async function getSystemInstructions(): Promise<string | null> {
  const now = Date.now()
  if (cachedSystemInstructions && now - cachedSystemInstructions.fetchedAt < INSTRUCTIONS_CACHE_TTL_MS) {
    console.log('cachedSystemInstructions', cachedSystemInstructions.text);
    return cachedSystemInstructions.text
  }
  try {
    const supabase = createSupabaseAdmin()
    const { data, error } = await supabase
      .from('app_settings')
      .select('system_instructions')
      .eq('id', 1)
      .maybeSingle()
    if (error) {
      console.warn('Failed to fetch system instructions, using default:', error)
      cachedSystemInstructions = { text: null, fetchedAt: now }
      return null
    }
    const text = data?.system_instructions || null
    cachedSystemInstructions = { text, fetchedAt: now }
    console.log('text', text);
    return text
  } catch (e) {
    console.warn('Error reading system instructions, using default:', e)
    cachedSystemInstructions = { text: null, fetchedAt: now }
    return null
  }
}

export interface ChatMessage {
  role: 'user' | 'assistant'
  content: string
}

export interface OpenAIResponse {
  content: string
  tokens_used?: number
  response_time_ms?: number
}

/**
 * OPTIMAL price query using Vector Store + File Search (no attachments)
 * Targets Venezuelan B2B price lookups across multiple catalogs
 */
export async function queryPricesFast(
  query: string,
  vectorStoreIds: string[],
  conversationHistory: ChatMessage[] = [],
  userIdentifier?: string | null
): Promise<OpenAIResponse> {
  const startTime = Date.now()

  try {
    // Build a per-request OpenAI client to attach user identity for Helicone analytics
    const userHeader = userIdentifier ? { 'Helicone-User-Id': userIdentifier } : undefined
    const openai = createOpenAIClient({ additionalHeaders: userHeader })

    if (vectorStoreIds.length === 0) {
      // Fallback: plain chat (no retrieval). Used when no vector stores are available.
      const systemPrompt = `Eres un asistente que responde preguntas bas√°ndote principalmente en los documentos y archivos proporcionados (PDF, DOCX, CSV, im√°genes, etc.).
Instrucciones:
- Prioriza la informaci√≥n que puedas recuperar de los archivos disponibles.
- Si no hay evidencia suficiente en los archivos, dilo expl√≠citamente y sugiere cargar/activar los documentos necesarios.
- Mant√©n las respuestas breves, claras y directas.
- No inventes datos ni hagas suposiciones.
- Cuando sea posible, menciona el archivo (o fuente) del que extraes la informaci√≥n.`

      const dynamicInstructions = await getSystemInstructions()
      const messages = [
        { role: 'system' as const, content: dynamicInstructions || systemPrompt },
        ...conversationHistory.map(msg => ({
          role: msg.role as 'user' | 'assistant',
          content: msg.content
        })),
        { role: 'user' as const, content: query }
      ]

      const response = await openai.chat.completions.create({
        model: 'gpt-4o-mini',
        messages,
        temperature: 0.3,
        max_tokens: 400
      })

      const processingTime = Date.now() - startTime
      console.log(`üí® Fallback query completed in ${processingTime}ms`)

      return {
        content: response.choices[0]?.message?.content || 'No se pudo procesar la consulta.',
        tokens_used: response.usage?.total_tokens,
        response_time_ms: processingTime,
      }
    }

    // OPTIMAL: Vector Stores + File Search via tool_resources. Used when vector stores are available.
    console.log(`üöÄ OPTIMAL SEARCH: Using ${vectorStoreIds.length} vector stores with intelligent retrieval`)

    const systemInstructions = `Instrucciones del asistente con recuperaci√≥n desde archivos

Eres un asistente que responde preguntas utilizando EXCLUSIVAMENTE la informaci√≥n disponible en los archivos conectados (PDF, DOCX, CSV, im√°genes con OCR, etc.). Si la evidencia no existe en los archivos, dilo claramente y sugiere qu√© documento cargar o activar.

IMPORTANTE: Los archivos proporcionados est√°n disponibles a trav√©s de la herramienta de b√∫squeda de archivos. SIEMPRE debes usar la herramienta file_search para buscar informaci√≥n en los archivos antes de responder. No asumas que no hay informaci√≥n sin haber buscado primero.

Reglas de b√∫squeda
  1. SIEMPRE usa la herramienta file_search para buscar en los archivos disponibles.
  2. Busca en TODOS los archivos y vector stores disponibles en la herramienta.
  3. Considera variaciones, sin√≥nimos y t√©rminos relacionados de la consulta.
  4. Cuando extraigas informaci√≥n, intenta mencionar el nombre del archivo (sin extensi√≥n) como fuente.
  5. Si la b√∫squeda no devuelve resultados, intenta con t√©rminos alternativos o m√°s generales.

Cobertura (obligatoria)
  1. Cubre todas las fuentes relevantes. Si una fuente no tiene coincidencias, ind√≠calo.
  2. No mezcles datos de distintas fuentes sin aclarar su origen.
  3. Si no encuentras informaci√≥n despu√©s de buscar, di claramente "No encontr√© informaci√≥n sobre [tema] en los archivos proporcionados".

Formato de respuesta (Markdown)
  ‚Ä¢ Responde en el idioma de la consulta (espa√±ol por defecto).
  ‚Ä¢ Agrupa por archivo/origen en orden alfab√©tico:

[Nombre del Archivo]
  ‚Ä¢ [Hallazgo/Hecho clave]
  [Cita/par√°frasis breve si aplica]

  ‚Ä¢ Evita tablas salvo que el usuario las pida.
  ‚Ä¢ S√© breve y claro; usa vi√±etas.

Pol√≠ticas
  ‚Ä¢ No inventes datos ni asumas valores no presentes.
  ‚Ä¢ Si la evidencia es insuficiente, explica qu√© falta y qu√© cargar.
  ‚Ä¢ No expongas informaci√≥n sensible que no est√© expl√≠citamente en los archivos.
  ‚Ä¢ SIEMPRE busca en los archivos usando la herramienta antes de decir que no hay informaci√≥n.`

    // Build messages preserving roles
    const dynamicInstructions = await getSystemInstructions()
    const inputMessages = [
      { role: 'system' as const, content: dynamicInstructions || systemInstructions },
      ...conversationHistory.map(msg => ({
        role: msg.role as 'user' | 'assistant',
        content: msg.content
      })),
      { role: 'user' as const, content: query }
    ]

    console.log('üì® OpenAI Request:')
    console.log(`  Model: gpt-4o-mini`)
    console.log(`  Vector Stores: ${vectorStoreIds.join(', ')}`)
    console.log(`  Messages: ${inputMessages.length} messages`)
    console.log(`  Query: "${query}"`)
    console.log(`  Tool: file_search with ${vectorStoreIds.length} vector store(s)`)

    const response = await openai.responses.create({
      model: 'gpt-4o-mini',
      input: inputMessages,
      tools: [
        {
          type: 'file_search',
          vector_store_ids: vectorStoreIds, // <-- required by the TS type
          // optional:
          // max_num_results: 12,
        }
      ],
      tool_choice: 'auto',     // let it decide when to retrieve
      temperature: 0.1,
      max_output_tokens: 400,
      stream: false
    })
    
    console.log(`üì• OpenAI Response (initial):`)
    console.log(`  Status: ${response.status}`)
    console.log(`  ID: ${response.id}`)

    // Poll if needed (Responses can be async)
    let finalResponse = response
    if (response.status === 'in_progress') {
      console.log(`‚è≥ Response is in_progress, polling...`)
      let attempts = 0
      const maxAttempts = 30 // up to ~15s
      while (finalResponse.status === 'in_progress' && attempts < maxAttempts) {
        await new Promise(r => setTimeout(r, 500))
        finalResponse = await openai.responses.retrieve(response.id)
        attempts++
        if (attempts % 5 === 0) {
          console.log(`  Polling attempt ${attempts}/${maxAttempts}, status: ${finalResponse.status}`)
        }
      }
    }

    if (finalResponse.status === 'completed') {
      const processingTime = Date.now() - startTime
      const performanceStatus =
        processingTime <= 60 ? 'EXCELLENT üöÄ' : processingTime <= 120 ? 'GOOD ‚ö°' : 'ACCEPTABLE ‚è∞'

      // Log detailed output structure
      console.log(`üì• OpenAI Response (final):`)
      console.log(`  Status: ${finalResponse.status}`)
      console.log(`  Output items: ${finalResponse.output?.length || 0}`)
      
      // Log file_search calls and results
      if (Array.isArray(finalResponse.output)) {
        finalResponse.output.forEach((item, idx) => {
          if (item.type === 'file_search_call') {
            console.log(`  [${idx}] File Search Call:`)
            console.log(`    Status: ${item.status}`)
            console.log(`    Queries: ${JSON.stringify(item.queries || [])}`)
            console.log(`    Results: ${item.results ? `${item.results.length} results` : 'null'}`)
            if (item.results && Array.isArray(item.results)) {
              console.log(`    Result details: ${item.results.map((r: any) => r.file_id || r.id || 'unknown').join(', ')}`)
            }
          } else if (item.type === 'message') {
            console.log(`  [${idx}] Message:`)
            console.log(`    Status: ${item.status}`)
            console.log(`    Content items: ${item.content?.length || 0}`)
          }
        })
      }

      // Extract text safely
      let content = 'No se pudo procesar la consulta.'
      if (Array.isArray(finalResponse.output) && finalResponse.output.length > 0) {
        const last = finalResponse.output[finalResponse.output.length - 1]
        if (last?.type === 'message') {
          const textPart = last.content?.find?.(c => c.type === 'output_text') as any
          if (textPart?.text) content = textPart.text
        }
      }

      console.log(`üéâ OPTIMAL SEARCH completed in ${processingTime}ms - ${performanceStatus}`)
      console.log(`üìä Processed ${vectorStoreIds.length} vector stores | tokens=${finalResponse.usage?.total_tokens ?? 0}`)
      console.log(`üìù Response content length: ${content.length} characters`)

      return {
        content,
        tokens_used: finalResponse.usage?.total_tokens,
        response_time_ms: processingTime,
      }
    } else {
      const processingTime = Date.now() - startTime
      console.error(`‚ùå Optimal search failed with status: ${finalResponse.status}`)
      console.error(`‚ùå Full response:`, JSON.stringify(finalResponse, null, 2))
      throw new Error(`La consulta no pudo completarse. Estado: ${finalResponse.status}`)
    }

  } catch (error) {
    const processingTime = Date.now() - startTime
    console.error(`‚ùå Optimal search failed after ${processingTime}ms:`, error)
    throw new Error(`Error en consulta de archivos: ${error instanceof Error ? error.message : 'Error desconocido'}`)
  }
}